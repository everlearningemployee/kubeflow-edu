{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 201016 01:57:31 config:123] Using preprocessor: <kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7f4b3c70bda0>\n",
      "[I 201016 01:57:31 config:125] Using builder: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f4af4d6bb00>\n",
      "[I 201016 01:57:31 config:127] Using deployer: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f4af4d6bb00>\n",
      "[W 201016 01:57:31 append:50] Building image using Append builder...\n",
      "[I 201016 01:57:31 base:105] Creating docker context: /tmp/fairing_context_niqrzqv1\n",
      "[I 201016 01:57:31 converted_notebook:127] Converting 02-train-fairing.ipynb to 02-train-fairing.py\n",
      "[I 201016 01:57:31 docker_creds_:234] Loading Docker credentials for repository 'brightfly/kubeflow-jupyter-lab:tf2.0-cpu'\n",
      "[W 201016 01:57:34 append:54] Image successfully built in 2.2638202610000917s.\n",
      "[W 201016 01:57:34 append:94] Pushing image kubeflow-registry.default.svc.cluster.local:30000/tensorboard-job:2BCB9C7F...\n",
      "[I 201016 01:57:34 docker_creds_:234] Loading Docker credentials for repository 'kubeflow-registry.default.svc.cluster.local:30000/tensorboard-job:2BCB9C7F'\n",
      "[W 201016 01:57:34 append:81] Uploading kubeflow-registry.default.svc.cluster.local:30000/tensorboard-job:2BCB9C7F\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:d83811f270d56d34a208f721f3dbf1b9242d1900ad8981fc7071339681998a31 pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:af228bb15b867acbee0d6df28dbb42a8218948fdde64833bd021a255a99a2690 pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:6ac505cbabeccba798e608e1bb80ae66f497c2c856733800e1db060e3c875917 pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:7acbfba21e38804c20c4da292d2f99d88d317b83440fb2f3de0a620edf2b9595 pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:d0d480fe0d0ab233a90471379d9d3789d4153ad4611d101cfefdfc0c507d9356 pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:ee671aafb583e2321880e275c94d49a49185006730e871435cd851f42d2a775d pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:d846ead9aa38bc0d35dd500ada5349fe42efb96c89db08b8abcb9ad949332c2a pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:fd781ca227d0199735cceb67afa81e2a073b5ef5867ca6e96ad46e002a03252a pushed.\n",
      "[I 201016 01:57:35 docker_session_:284] Layer sha256:2e9d21328b88847f4e93540ea928bfee9d4cfaebd6f8cf7481bf3c344328206a pushed.\n",
      "[I 201016 01:57:36 docker_session_:284] Layer sha256:069de100aaecb1299b09dab29a4eb39a2ca6a5f8c0a033b65ff0e15e9ea39a24 pushed.\n",
      "[I 201016 01:57:36 docker_session_:284] Layer sha256:df80607e3e38dc3f117e967a36620c42f791afc9ca0568900d13d84c1e3a38bb pushed.\n",
      "[I 201016 01:57:36 docker_session_:284] Layer sha256:50f1fe304a9431ea025aa6ee5e8e45826fa62640a83e55be8bd3c227119cbcd2 pushed.\n",
      "[I 201016 01:57:36 docker_session_:284] Layer sha256:5667fdb72017d1fb364744ca1abf7b6f3bbe9c98c3786f294a461c2866db69ab pushed.\n",
      "[I 201016 01:57:36 docker_session_:284] Layer sha256:872efc07c3d848e0f55b7cbce9866a8dea7b76d8c1f131801934ea0d856b15c5 pushed.\n",
      "[I 201016 01:57:36 docker_session_:284] Layer sha256:40d1e4b966206ffd3cf3c4ed8ceaf5d12eff3b41b874221bf244d9243edf0e7b pushed.\n",
      "[I 201016 01:57:37 docker_session_:284] Layer sha256:7fc152dfb3a6b5c9a436b49ff6cd72ed7eb5f1fd349128b50ee04c3c5c2355fb pushed.\n",
      "[I 201016 01:57:37 docker_session_:284] Layer sha256:fb9676c4102241f220b864e98c53676e27c407c6ecfbb13c2cbc1fa4de8f1811 pushed.\n",
      "[I 201016 01:57:37 docker_session_:284] Layer sha256:66257906239d377cd700c566b27f12895ccefcd8d95eae7377f600208151d8e0 pushed.\n",
      "[I 201016 01:57:37 docker_session_:284] Layer sha256:1c60aaf19cc4c142934f390281975f61956475c205b5895711f839a91f927779 pushed.\n",
      "[I 201016 01:57:37 docker_session_:284] Layer sha256:485ea80d093de507a5ccd36effaf8dad03b67a1063edbd427fedfe667c9155bc pushed.\n",
      "[I 201016 01:57:38 docker_session_:284] Layer sha256:8f64ecfab6c102a62dd5d912d96113603663ad8b436701bf2dbd1aa9630882e7 pushed.\n",
      "[I 201016 01:57:41 docker_session_:284] Layer sha256:8e7d3ffcd500d3ed0b2ee2449a36a44601b8e282cc6d541ab93f1189d4a2cf93 pushed.\n",
      "[I 201016 01:57:41 docker_session_:284] Layer sha256:c2f81144f815902f9c6e6f7883068544e35924f0efe2275bc41c3d0c558f394c pushed.\n",
      "[I 201016 01:57:41 docker_session_:334] Finished upload of: kubeflow-registry.default.svc.cluster.local:30000/tensorboard-job:2BCB9C7F\n",
      "[W 201016 01:57:41 append:99] Pushed image kubeflow-registry.default.svc.cluster.local:30000/tensorboard-job:2BCB9C7F in 6.987285664999945s.\n",
      "[W 201016 01:57:41 job:90] The job fairing-job-k28hv launched.\n",
      "[W 201016 01:57:41 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:57:41 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:57:41 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:57:42 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:57:43 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:57:44 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:58:12 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:58:27 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:58:38 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:58:50 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:59:26 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 01:59:38 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:00:56 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:01:09 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:03:48 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:04:02 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:07:40 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:07:40 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:07:41 manager:227] Waiting for fairing-job-k28hv-z4nmk to start...\n",
      "[W 201016 02:07:41 manager:227] Waiting for fairing-job-k28hv-z4nmk to start...\n",
      "[W 201016 02:07:41 manager:227] Waiting for fairing-job-k28hv-z4nmk to start...\n",
      "[W 201016 02:07:42 manager:227] Waiting for fairing-job-k28hv-z4nmk to start...\n",
      "[W 201016 02:07:42 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[W 201016 02:07:42 manager:227] Waiting for fairing-job-k28hv-xlf8f to start...\n",
      "[I 201016 02:08:52 manager:233] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "2020-10-16 02:08:55.118121: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2020-10-16 02:08:55.125385: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000125000 Hz\n",
      "2020-10-16 02:08:55.126209: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5aca3e0 executing computations on platform Host. Devices:\n",
      "2020-10-16 02:08:55.126245: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0\n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480\n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0\n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290\n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      " 2016/48000 [>.............................] - ETA: 22s - loss: 1.2254 - accuracy: 0.63872\n",
      " 4640/48000 [=>............................] - ETA: 14s - loss: 0.8414 - accuracy: 0.744\n",
      " 7296/48000 [===>..........................] - ETA: 12s - loss: 0.7093 - accuracy: 0.794\n",
      " 9600/48000 [=====>........................] - ETA: 10s - loss: 0.6181 - accuracy: 0.827\n",
      "12192/48000 [======>.......................] - ETA: 9s - loss: 0.5611 - accuracy: 0.8320\n",
      "14944/48000 [========>.....................] - ETA: 8s - loss: 0.5184 - accuracy: 0.85\n",
      "17632/48000 [==========>...................] - ETA: 7s - loss: 0.4828 - accuracy: 0.86\n",
      "19840/48000 [===========>..................] - ETA: 7s - loss: 0.4564 - accuracy: 0.867\n",
      "22528/48000 [=============>................] - ETA: 6s - loss: 0.4333 - accuracy: 0.874\n",
      "25280/48000 [==============>...............] - ETA: 5s - loss: 0.4166 - accuracy: 0.879\n",
      "28064/48000 [================>.............] - ETA: 4s - loss: 0.4034 - accuracy: 0.88\n",
      "30752/48000 [=================>............] - ETA: 4s - loss: 0.3869 - accuracy: 0.88\n",
      "33024/48000 [===================>..........] - ETA: 3s - loss: 0.3744 - accuracy: 0.89\n",
      "35616/48000 [=====================>........] - ETA: 2s - loss: 0.3618 - accuracy: 0.89\n",
      "38368/48000 [======================>.......] - ETA: 2s - loss: 0.3511 - accuracy: 0.89\n",
      "41024/48000 [========================>.....] - ETA: 1s - loss: 0.3436 - accuracy: 0.90\n",
      "43296/48000 [==========================>...] - ETA: 1s - loss: 0.3351 - accuracy: 0.90\n",
      "45984/48000 [===========================>..] - ETA: 0s - loss: 0.3278 - accuracy: 0.904\n",
      "48000/48000 [==============================] - 13s 267us/sample - loss: 0.3220 - accuracy: 0.9065 - val_loss: 0.1503 - val_accuracy: 0.9588\n",
      "Epoch 2/3\n",
      " 1952/48000 [>.............................] - ETA: 11s - loss: 0.1571 - accuracy: 0.958\n",
      " 4608/48000 [=>............................] - ETA: 10s - loss: 0.1644 - accuracy: 0.951\n",
      " 7232/48000 [===>..........................] - ETA: 9s - loss: 0.1651 - accuracy: 0.940\n",
      " 9536/48000 [====>.........................] - ETA: 8s - loss: 0.1700 - accuracy: 0.95\n",
      "12224/48000 [======>.......................] - ETA: 8s - loss: 0.1712 - accuracy: 0.95\n",
      "14880/48000 [========>.....................] - ETA: 7s - loss: 0.1690 - accuracy: 0.95\n",
      "17536/48000 [=========>....................] - ETA: 6s - loss: 0.1649 - accuracy: 0.95\n",
      "20256/48000 [===========>..................] - ETA: 6s - loss: 0.1611 - accuracy: 0.95\n",
      "22496/48000 [=============>................] - ETA: 5s - loss: 0.1602 - accuracy: 0.953\n",
      "25120/48000 [==============>...............] - ETA: 5s - loss: 0.1574 - accuracy: 0.954\n",
      "27840/48000 [================>.............] - ETA: 4s - loss: 0.1580 - accuracy: 0.953\n",
      "30432/48000 [==================>...........] - ETA: 4s - loss: 0.1584 - accuracy: 0.95\n",
      "33184/48000 [===================>..........] - ETA: 3s - loss: 0.1590 - accuracy: 0.95\n",
      "35424/48000 [=====================>........] - ETA: 2s - loss: 0.1585 - accuracy: 0.95\n",
      "38144/48000 [======================>.......] - ETA: 2s - loss: 0.1588 - accuracy: 0.95\n",
      "40864/48000 [========================>.....] - ETA: 1s - loss: 0.1579 - accuracy: 0.95\n",
      "43488/48000 [=========================>....] - ETA: 1s - loss: 0.1569 - accuracy: 0.95\n",
      "45536/48000 [===========================>..] - ETA: 0s - loss: 0.1558 - accuracy: 0.95\n",
      "48000/48000 [==============================] - 12s 258us/sample - loss: 0.1547 - accuracy: 0.9539 - val_loss: 0.1165 - val_accuracy: 0.9638\n",
      "\n",
      "Epoch 3/3\n",
      " 1952/48000 [>.............................] - ETA: 11s - loss: 0.1202 - accuracy: 0.966\n",
      " 4608/48000 [=>............................] - ETA: 10s - loss: 0.1263 - accuracy: 0.963\n",
      " 7232/48000 [===>..........................] - ETA: 9s - loss: 0.1168 - accuracy: 0.9654\n",
      " 9440/48000 [====>.........................] - ETA: 8s - loss: 0.1206 - accuracy: 0.96\n",
      "12064/48000 [======>.......................] - ETA: 8s - loss: 0.1204 - accuracy: 0.96\n",
      "14592/48000 [========>.....................] - ETA: 7s - loss: 0.1218 - accuracy: 0.96\n",
      "17344/48000 [=========>....................] - ETA: 7s - loss: 0.1229 - accuracy: 0.96\n",
      "20000/48000 [===========>..................] - ETA: 6s - loss: 0.1210 - accuracy: 0.96\n",
      "22272/48000 [============>.................] - ETA: 5s - loss: 0.1179 - accuracy: 0.965\n",
      "24960/48000 [==============>...............] - ETA: 5s - loss: 0.1185 - accuracy: 0.965\n",
      "27712/48000 [================>.............] - ETA: 4s - loss: 0.1172 - accuracy: 0.965\n",
      "30400/48000 [==================>...........] - ETA: 4s - loss: 0.1172 - accuracy: 0.96\n",
      "33184/48000 [===================>..........] - ETA: 3s - loss: 0.1165 - accuracy: 0.96\n",
      "35392/48000 [=====================>........] - ETA: 2s - loss: 0.1157 - accuracy: 0.96\n",
      "38080/48000 [======================>.......] - ETA: 2s - loss: 0.1160 - accuracy: 0.96\n",
      "40736/48000 [========================>.....] - ETA: 1s - loss: 0.1153 - accuracy: 0.96\n",
      "43424/48000 [=========================>....] - ETA: 1s - loss: 0.1156 - accuracy: 0.96\n",
      "45632/48000 [===========================>..] - ETA: 0s - loss: 0.1147 - accuracy: 0.966\n",
      "48000/48000 [==============================] - 12s 256us/sample - loss: 0.1146 - accuracy: 0.9660 - val_loss: 0.0973 - val_accuracy: 0.97\n",
      "03\n",
      "Test accuracy:  0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 201016 02:09:35 job:162] Cleaning up job fairing-job-k28hv...\n"
     ]
    }
   ],
   "source": [
    "# add for tensorboard code\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import argparse\n",
    "import json\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# origin fairing code\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class MyFashionMnist(object):\n",
    "  def train(self):\n",
    "           \n",
    "    print(\"TensorFlow version: \", tf.__version__)\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=3, validation_split=0.2 ) \n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, batch_size=128, verbose=0)\n",
    "    print('Test accuracy: ', score[1])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "        from kubeflow import fairing\n",
    "        from kubeflow.fairing.kubernetes import utils as k8s_utils\n",
    "        \n",
    "        DOCKER_REGISTRY = 'kubeflow-registry.default.svc.cluster.local:30000'   # 프라이빗 레지스트리\n",
    "\n",
    "        fairing.config.set_builder(\n",
    "            'append',\n",
    "            image_name='tensorboard-job', # here not fairing job but katib job\n",
    "            base_image='brightfly/kubeflow-jupyter-lab:tf2.0-cpu',\n",
    "            registry=DOCKER_REGISTRY, \n",
    "            push=True\n",
    "        )\n",
    "        # cpu 1, memory 5GiB\n",
    "        fairing.config.set_deployer('job',\n",
    "                                    namespace='admin', # here\n",
    "                                    pod_spec_mutators=[\n",
    "                                        k8s_utils.get_resource_mutator(cpu=1,  # here\n",
    "                                                                       memory=5)]\n",
    "                                   )\n",
    "        fairing.config.run()\n",
    "    else:\n",
    "        remote_train = MyFashionMnist()\n",
    "        remote_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
